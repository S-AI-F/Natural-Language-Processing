<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Text classification | Natural Language Processing with R</title>
  <meta name="description" content="This is a tutorial of various techniques used in natural language processing and text mining." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Text classification | Natural Language Processing with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a tutorial of various techniques used in natural language processing and text mining." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Text classification | Natural Language Processing with R" />
  
  <meta name="twitter:description" content="This is a tutorial of various techniques used in natural language processing and text mining." />
  

<meta name="author" content="Saif SHabou" />


<meta name="date" content="2020-05-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Word-embeddings.html"/>
<link rel="next" href="RNN.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">NLP with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="text-processing.html"><a href="text-processing.html"><i class="fa fa-check"></i><b>2</b> Text processing</a><ul>
<li class="chapter" data-level="2.1" data-path="text-processing.html"><a href="text-processing.html#text-data"><i class="fa fa-check"></i><b>2.1</b> Text data</a></li>
<li class="chapter" data-level="2.2" data-path="text-processing.html"><a href="text-processing.html#nlp-applications"><i class="fa fa-check"></i><b>2.2</b> NLP applications</a></li>
<li class="chapter" data-level="2.3" data-path="text-processing.html"><a href="text-processing.html#tokenization"><i class="fa fa-check"></i><b>2.3</b> Tokenization</a></li>
<li class="chapter" data-level="2.4" data-path="text-processing.html"><a href="text-processing.html#stop-words-handeling"><i class="fa fa-check"></i><b>2.4</b> Stop words handeling</a></li>
<li class="chapter" data-level="2.5" data-path="text-processing.html"><a href="text-processing.html#words-frequencies"><i class="fa fa-check"></i><b>2.5</b> Words frequencies</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Word-embeddings.html"><a href="Word-embeddings.html"><i class="fa fa-check"></i><b>3</b> Word embeddings</a><ul>
<li class="chapter" data-level="3.1" data-path="Word-embeddings.html"><a href="Word-embeddings.html#vectorizing-text"><i class="fa fa-check"></i><b>3.1</b> Vectorizing text</a></li>
<li class="chapter" data-level="3.2" data-path="Word-embeddings.html"><a href="Word-embeddings.html#one-hot-encoding"><i class="fa fa-check"></i><b>3.2</b> One-hot encoding</a></li>
<li class="chapter" data-level="3.3" data-path="Word-embeddings.html"><a href="Word-embeddings.html#word-embeddings-methods"><i class="fa fa-check"></i><b>3.3</b> Word embeddings methods</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Word-embeddings.html"><a href="Word-embeddings.html#learn-world-embeddings"><i class="fa fa-check"></i><b>3.3.1</b> Learn world embeddings</a></li>
<li class="chapter" data-level="3.3.2" data-path="Word-embeddings.html"><a href="Word-embeddings.html#pre-trained-word-embeddings"><i class="fa fa-check"></i><b>3.3.2</b> Pre-trained word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Word-embeddings.html"><a href="Word-embeddings.html#applications"><i class="fa fa-check"></i><b>3.4</b> Applications</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Word-embeddings.html"><a href="Word-embeddings.html#using-skip-gram"><i class="fa fa-check"></i><b>3.4.1</b> Using Skip-Gram</a></li>
<li class="chapter" data-level="3.4.2" data-path="Word-embeddings.html"><a href="Word-embeddings.html#using-glove"><i class="fa fa-check"></i><b>3.4.2</b> Using GloVe</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="Word-embeddings.html"><a href="Word-embeddings.html#references"><i class="fa fa-check"></i><b>3.5</b> references</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="text-classification.html"><a href="text-classification.html"><i class="fa fa-check"></i><b>4</b> Text classification</a><ul>
<li class="chapter" data-level="4.1" data-path="text-classification.html"><a href="text-classification.html#load-the-data"><i class="fa fa-check"></i><b>4.1</b> Load the data</a></li>
<li class="chapter" data-level="4.2" data-path="text-classification.html"><a href="text-classification.html#prepare-the-data-for-neural-network"><i class="fa fa-check"></i><b>4.2</b> Prepare the data for neural network</a></li>
<li class="chapter" data-level="4.3" data-path="text-classification.html"><a href="text-classification.html#building-the-model"><i class="fa fa-check"></i><b>4.3</b> Building the model</a></li>
<li class="chapter" data-level="4.4" data-path="text-classification.html"><a href="text-classification.html#testing-the-model"><i class="fa fa-check"></i><b>4.4</b> Testing the model</a></li>
<li class="chapter" data-level="4.5" data-path="text-classification.html"><a href="text-classification.html#reference"><i class="fa fa-check"></i><b>4.5</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="RNN.html"><a href="RNN.html"><i class="fa fa-check"></i><b>5</b> Reccurent Neural Networks (RNN)</a><ul>
<li class="chapter" data-level="5.1" data-path="RNN.html"><a href="RNN.html#understanding-recurrent-neural-network"><i class="fa fa-check"></i><b>5.1</b> Understanding Recurrent Neural Network</a></li>
<li class="chapter" data-level="5.2" data-path="RNN.html"><a href="RNN.html#rnn-with-keras"><i class="fa fa-check"></i><b>5.2</b> RNN with Keras</a></li>
<li class="chapter" data-level="5.3" data-path="RNN.html"><a href="RNN.html#lstm-with-keras"><i class="fa fa-check"></i><b>5.3</b> LSTM with Keras</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html"><i class="fa fa-check"></i><b>6</b> Sentiment Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#the-sentiments-dataset"><i class="fa fa-check"></i><b>6.1</b> The “Sentiments” dataset</a></li>
<li class="chapter" data-level="6.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#application"><i class="fa fa-check"></i><b>6.2</b> Application</a></li>
<li class="chapter" data-level="6.3" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#references-1"><i class="fa fa-check"></i><b>6.3</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="word-and-document-frequency-tf-idf.html"><a href="word-and-document-frequency-tf-idf.html"><i class="fa fa-check"></i><b>7</b> Word and document frequency (TF-IDF)</a><ul>
<li class="chapter" data-level="7.1" data-path="word-and-document-frequency-tf-idf.html"><a href="word-and-document-frequency-tf-idf.html#term-frequency-application"><i class="fa fa-check"></i><b>7.1</b> Term frequency application</a></li>
<li class="chapter" data-level="7.2" data-path="word-and-document-frequency-tf-idf.html"><a href="word-and-document-frequency-tf-idf.html#zipfs-law"><i class="fa fa-check"></i><b>7.2</b> Zipf’s law</a></li>
<li class="chapter" data-level="7.3" data-path="word-and-document-frequency-tf-idf.html"><a href="word-and-document-frequency-tf-idf.html#tf_idf-metric"><i class="fa fa-check"></i><b>7.3</b> TF_IDF metric</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="topic-modeling.html"><a href="topic-modeling.html"><i class="fa fa-check"></i><b>8</b> Topic modeling</a><ul>
<li class="chapter" data-level="8.1" data-path="topic-modeling.html"><a href="topic-modeling.html#latent-dirichlet-allocation"><i class="fa fa-check"></i><b>8.1</b> Latent Dirichlet allocation</a></li>
<li class="chapter" data-level="8.2" data-path="topic-modeling.html"><a href="topic-modeling.html#document-topic-probabilities"><i class="fa fa-check"></i><b>8.2</b> Document-topic probabilities</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="words-relationships-analysis.html"><a href="words-relationships-analysis.html"><i class="fa fa-check"></i><b>9</b> Words’ relationships analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="words-relationships-analysis.html"><a href="words-relationships-analysis.html#extracting-bi-grams"><i class="fa fa-check"></i><b>9.1</b> Extracting bi-grams</a></li>
<li class="chapter" data-level="9.2" data-path="words-relationships-analysis.html"><a href="words-relationships-analysis.html#analyzing-bi-grams"><i class="fa fa-check"></i><b>9.2</b> Analyzing bi-grams</a></li>
<li class="chapter" data-level="9.3" data-path="words-relationships-analysis.html"><a href="words-relationships-analysis.html#visualizing-a-network-of-bigrams"><i class="fa fa-check"></i><b>9.3</b> Visualizing a network of bigrams</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="document-term-matrix.html"><a href="document-term-matrix.html"><i class="fa fa-check"></i><b>10</b> Document-term matrix</a><ul>
<li class="chapter" data-level="10.1" data-path="document-term-matrix.html"><a href="document-term-matrix.html#converting-dtm-into-dataframe"><i class="fa fa-check"></i><b>10.1</b> COnverting DTM into dataframe</a></li>
<li class="chapter" data-level="10.2" data-path="document-term-matrix.html"><a href="document-term-matrix.html#generating-document-term-matrix"><i class="fa fa-check"></i><b>10.2</b> Generating Document-term matrix</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Natural Language Processing with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="text-classification" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Text classification</h1>
<p>This tutorial classifies movie reviews as positive or negative using the text of the review.
We’ll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" title="1"><span class="kw">library</span>(keras)</a>
<a class="sourceLine" id="cb76-2" title="2"><span class="kw">library</span>(tensorflow)</a></code></pre></div>
<div id="load-the-data" class="section level2">
<h2><span class="header-section-number">4.1</span> Load the data</h2>
<p>We will keep only the op 10,000 most frequently occuring words in the training data.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" title="1">imdb &lt;-<span class="st"> </span><span class="kw">dataset_imdb</span>(<span class="dt">num_words =</span> <span class="dv">10000</span>)</a>
<a class="sourceLine" id="cb77-2" title="2">train_data &lt;-<span class="st"> </span>imdb<span class="op">$</span>train<span class="op">$</span>x</a>
<a class="sourceLine" id="cb77-3" title="3">train_labels &lt;-<span class="st"> </span>imdb<span class="op">$</span>train<span class="op">$</span>y</a>
<a class="sourceLine" id="cb77-4" title="4">test_data &lt;-<span class="st"> </span>imdb<span class="op">$</span>test<span class="op">$</span>x</a>
<a class="sourceLine" id="cb77-5" title="5">test_labels &lt;-<span class="st"> </span>imdb<span class="op">$</span>test<span class="op">$</span>y</a></code></pre></div>
<p>The obtained <code>train_data</code> and <code>test_data</code> are lists of review. Each review is a list of word indices.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" title="1"><span class="kw">str</span>(train_data[[<span class="dv">1</span>]])</a></code></pre></div>
<pre><code>##  int [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...</code></pre>
<p>The obtained <code>train_labels</code> and <code>test_labels</code> are lists of 0 (negatove revieuw) and 1 (positive review).</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" title="1"><span class="kw">str</span>(train_labels[[<span class="dv">1</span>]])</a></code></pre></div>
<pre><code>##  int 1</code></pre>
<p>We can decode the words index back to text words in this way:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" title="1"><span class="co"># Named list mapping words to an integer index.</span></a>
<a class="sourceLine" id="cb82-2" title="2">word_index &lt;-<span class="st"> </span><span class="kw">dataset_imdb_word_index</span>()  </a>
<a class="sourceLine" id="cb82-3" title="3">reverse_word_index &lt;-<span class="st"> </span><span class="kw">names</span>(word_index)</a>
<a class="sourceLine" id="cb82-4" title="4"><span class="kw">names</span>(reverse_word_index) &lt;-<span class="st"> </span>word_index</a>
<a class="sourceLine" id="cb82-5" title="5"></a>
<a class="sourceLine" id="cb82-6" title="6"><span class="co"># Decodes the review. Note that the indices are offset by 3 because 0, 1, and </span></a>
<a class="sourceLine" id="cb82-7" title="7"><span class="co"># 2 are reserved indices for &quot;padding,&quot; &quot;start of sequence,&quot; and &quot;unknown.&quot;</span></a>
<a class="sourceLine" id="cb82-8" title="8">decoded_review &lt;-<span class="st"> </span><span class="kw">sapply</span>(train_data[[<span class="dv">1</span>]], <span class="cf">function</span>(index) {</a>
<a class="sourceLine" id="cb82-9" title="9">  word &lt;-<span class="st"> </span><span class="cf">if</span> (index <span class="op">&gt;=</span><span class="st"> </span><span class="dv">3</span>) reverse_word_index[[<span class="kw">as.character</span>(index <span class="op">-</span><span class="st"> </span><span class="dv">3</span>)]]</a>
<a class="sourceLine" id="cb82-10" title="10">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(word)) word <span class="cf">else</span> <span class="st">&quot;?&quot;</span></a>
<a class="sourceLine" id="cb82-11" title="11">})</a>
<a class="sourceLine" id="cb82-12" title="12"><span class="kw">cat</span>(decoded_review)</a></code></pre></div>
<pre><code>## ? this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&#39;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all</code></pre>
</div>
<div id="prepare-the-data-for-neural-network" class="section level2">
<h2><span class="header-section-number">4.2</span> Prepare the data for neural network</h2>
<p>Since we can’t feed a list of integers into a neural network, we need to transfomr our ists into tensors. There are two options to turn lists of integers into tensors:</p>
<ul>
<li>Pad our lists in way that they have the same length. We pad them into an integer tensor of shape <code>samples, word_indices</code>. Then, we use a first layer in our network a layer that can handle such integer tensors, like the <code>embedding_layer</code>.</li>
<li>One-hot encoding our lists by transforming them into vectors of 0s and 1s. Then, we could use the obtained sparce vectors as the first layer. We will test this solution in the tutorial in order to learn how to vectorize manually the data.</li>
</ul>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" title="1">vectorize_sequences &lt;-<span class="st"> </span><span class="cf">function</span>(sequences, <span class="dt">dimension =</span> <span class="dv">10000</span>) {</a>
<a class="sourceLine" id="cb84-2" title="2">  <span class="co"># Creates an all-zero matrix of shape (length(sequences), dimension)</span></a>
<a class="sourceLine" id="cb84-3" title="3">  results &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> <span class="kw">length</span>(sequences), <span class="dt">ncol =</span> dimension) </a>
<a class="sourceLine" id="cb84-4" title="4">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(sequences))</a>
<a class="sourceLine" id="cb84-5" title="5">    <span class="co"># Sets specific indices of results[i] to 1s</span></a>
<a class="sourceLine" id="cb84-6" title="6">    results[i, sequences[[i]]] &lt;-<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb84-7" title="7">  results</a>
<a class="sourceLine" id="cb84-8" title="8">}</a>
<a class="sourceLine" id="cb84-9" title="9"></a>
<a class="sourceLine" id="cb84-10" title="10">x_train &lt;-<span class="st"> </span><span class="kw">vectorize_sequences</span>(train_data)</a>
<a class="sourceLine" id="cb84-11" title="11">x_test &lt;-<span class="st"> </span><span class="kw">vectorize_sequences</span>(test_data)</a></code></pre></div>
<p>Here’s what the samples look like now:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" title="1"><span class="kw">str</span>(x_train[<span class="dv">1</span>,])</a></code></pre></div>
<pre><code>##  num [1:10000] 1 1 0 1 1 1 1 1 1 0 ...</code></pre>
<p>We should also convert your labels from integer to numeric</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" title="1">y_train &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(train_labels)</a>
<a class="sourceLine" id="cb87-2" title="2">y_test &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(test_labels)</a></code></pre></div>
</div>
<div id="building-the-model" class="section level2">
<h2><span class="header-section-number">4.3</span> Building the model</h2>
<p>We will use a simple stack of fully connected dense layers with <code>relu</code> activation.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1"><span class="kw">library</span>(keras)</a>
<a class="sourceLine" id="cb88-2" title="2"></a>
<a class="sourceLine" id="cb88-3" title="3">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb88-4" title="4"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">16</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">10000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb88-5" title="5"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">16</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb88-6" title="6"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</a></code></pre></div>
<p>We compile the model</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" title="1">model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</a>
<a class="sourceLine" id="cb89-2" title="2">  <span class="dt">optimizer =</span> <span class="kw">optimizer_rmsprop</span>(<span class="dt">lr=</span><span class="fl">0.001</span>),</a>
<a class="sourceLine" id="cb89-3" title="3">  <span class="dt">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</a>
<a class="sourceLine" id="cb89-4" title="4">  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&quot;accuracy&quot;</span>)</a>
<a class="sourceLine" id="cb89-5" title="5">)</a></code></pre></div>
<p>In order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" title="1">val_indices &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb90-2" title="2"></a>
<a class="sourceLine" id="cb90-3" title="3">x_val &lt;-<span class="st"> </span>x_train[val_indices,]</a>
<a class="sourceLine" id="cb90-4" title="4">partial_x_train &lt;-<span class="st"> </span>x_train[<span class="op">-</span>val_indices,]</a>
<a class="sourceLine" id="cb90-5" title="5"></a>
<a class="sourceLine" id="cb90-6" title="6">y_val &lt;-<span class="st"> </span>y_train[val_indices]</a>
<a class="sourceLine" id="cb90-7" title="7">partial_y_train &lt;-<span class="st"> </span>y_train[<span class="op">-</span>val_indices]</a></code></pre></div>
<p>Now we train the odel over 10 epochs, in mini-batches of 512 samples. In order to monitor loss ad accuracy on the validation set, we pass the validation data as <code>validation_data</code> argument.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" title="1">history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span>keras<span class="op">::</span><span class="kw">fit</span>(</a>
<a class="sourceLine" id="cb91-2" title="2">  partial_x_train,</a>
<a class="sourceLine" id="cb91-3" title="3">  partial_y_train,</a>
<a class="sourceLine" id="cb91-4" title="4">  <span class="dt">epochs =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb91-5" title="5">  <span class="dt">batch_size =</span> <span class="dv">512</span>,</a>
<a class="sourceLine" id="cb91-6" title="6">  <span class="dt">validation_data =</span> <span class="kw">list</span>(x_val, y_val)</a>
<a class="sourceLine" id="cb91-7" title="7">)</a>
<a class="sourceLine" id="cb91-8" title="8"><span class="kw">plot</span>(history)</a></code></pre></div>
</div>
<div id="testing-the-model" class="section level2">
<h2><span class="header-section-number">4.4</span> Testing the model</h2>
<p>We saw in the last section that the model performance decrease after 4 epochs and starts to overfitting. So, we can decide to stop training after 4 epochs to avoid overfitting. We will train the model from scratch with 4 epochs and evaluate it with the test data.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" title="1">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb92-2" title="2"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">16</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">10000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb92-3" title="3"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">16</span>, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb92-4" title="4"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</a>
<a class="sourceLine" id="cb92-5" title="5"></a>
<a class="sourceLine" id="cb92-6" title="6">model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</a>
<a class="sourceLine" id="cb92-7" title="7">  <span class="dt">optimizer =</span> <span class="st">&quot;rmsprop&quot;</span>,</a>
<a class="sourceLine" id="cb92-8" title="8">  <span class="dt">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</a>
<a class="sourceLine" id="cb92-9" title="9">  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&quot;accuracy&quot;</span>)</a>
<a class="sourceLine" id="cb92-10" title="10">)</a>
<a class="sourceLine" id="cb92-11" title="11"></a>
<a class="sourceLine" id="cb92-12" title="12">model <span class="op">%&gt;%</span><span class="st"> </span>keras<span class="op">::</span><span class="kw">fit</span>(x_train, y_train, <span class="dt">epochs =</span> <span class="dv">4</span>, <span class="dt">batch_size =</span> <span class="dv">512</span>)</a>
<a class="sourceLine" id="cb92-13" title="13">results &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">evaluate</span>(x_test, y_test)</a>
<a class="sourceLine" id="cb92-14" title="14">results</a></code></pre></div>
</div>
<div id="reference" class="section level2">
<h2><span class="header-section-number">4.5</span> Reference</h2>
<p>Chollet &amp; Allaire (2017, Dec. 7). RStudio AI Blog: Deep Learning for Text Classification with Keras. Retrieved from <a href="https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras/" class="uri">https://blogs.rstudio.com/tensorflow/posts/2017-12-07-text-classification-with-keras/</a></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Word-embeddings.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="RNN.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["NLP-book.pdf", "NLP-book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
